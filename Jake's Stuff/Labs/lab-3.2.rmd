---
format:
  html:
    embed-resources: true
jupyter: ir
---

# Lab-3.2: Non-linear
Author: Dr. Purna Gamage

**Instructions**

* Read and complete all exercises below in the provided `.rmd` notebook 
<!-- click here to download the notebook for the assignment --> 

**Submission:**

* You need to upload ONE document to Canvas when you are done. 
* A PDF (or HTML) of the completed form of this notebook
* The final uploaded version should NOT have any code-errors present. 
* All outputs must be visible in the uploaded version, including code-cell outputs, images, graphs, etc

```{r}
#| vscode: {languageId: r}
library(ISLR)
library(gam)
library(splines)
library(tidyverse)
library(caret)
```

# Part-1: Demonstration

## Polynomial Regression

### Example 1

Make a sequence of x values to build model matrices.
Also make a nonlinear response variable and store everything in a data frame.

```{r}
#| vscode: {languageId: r}
x = seq(-3,3,by=.01) #random numbers
y = cos(x) + rnorm(length(x))/2 #non-linear function
mydf = data.frame(x = x, y = y)
plot(mydf)
```

Make the model matrix for simple linear regression.

```{r}
#| vscode: {languageId: r}
X1 = model.matrix(y ~ x) #create a design matrix
print(head(X1))
```

Make a model matrix for polynomial regression.
The basis functions are the powers of the X variable.

```{r}
#| vscode: {languageId: r}
X6 = model.matrix(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6))
print(head(X6))
```

Also make a similar model matrix using the poly() function.
This also makes basis functions which are polynomials of increasing degree.
However, these basis functions have better numerical and statistical properties as we will see shortly.

```{r}
#| vscode: {languageId: r}
X6p = model.matrix(y ~ poly(x,6))
print(head(X6))
print(head(X6p)) #values are different
```

Make a plot of the basis functions x, x^2, ..., x^6.
Since these values are fairly large near the ends of the interval, we need a large ylim.

```{r}
#| vscode: {languageId: r}
plot(x,X6[,1], type = 'l', ylim = c(-50,50))
matlines(x,X6[,2:7],lwd = 2)
grid(col = 3) #plotting the polynomials with degree 1,..,6
```

Compute also the correlation coefficients.
As you can see, even powers are correlated with each other and odd powers are also correlated with each other.

```{r}
#| vscode: {languageId: r}
print(round(cor(X6[,2:7]),2)) 
#what can you say about the correlation?high?low? misfitting? if the features are correlated then what?==> multicollinearity ==>lead to numerical instability of fitting the model
```

By contrast, the basis function generated by poly() are not correlated at all.
They are "orthogonal polynomials".

```{r}
#| vscode: {languageId: r}
print(round(cor(X6p[,2:7]),2))
#what do you notice? poly() invokes othogonal polynomials. 
#donot want linear combinations of features that create other features, we want it to be othonormal basis sets ~ linear Algebra
```

The basis functions x, x^2, ... also have less desirable numerical properties.
In the matrix formulation of linear regression, the following matrix and its inverse appear:

```{r}
#| vscode: {languageId: r}
A = t(X6)%*%X6 #X^TX (XtransposeX) %*% - matrix multiplication
B = solve(A) #matrix inverse
```

Then A*B should be the identity matrix.
Let's compute the product and check how this differs from the identity matrix:

```{r}
#| vscode: {languageId: r}
max(abs(A%*%B -diag(rep(1,7)))) #this should be 0 if it is equal to the identity matrix
```

This should really be much smaller. However, for this particular problem, it does not cause trouble.

Now plot the orthogonal basis functions generated by poly().

```{r}
#| vscode: {languageId: r}
plot(x,X6p[,1], type = 'l', ylim = c(-2,2))
matlines(x,X6p[,2:7],lwd = 2)
grid(col = 3)
```

All basis functions are much more similar in magnitude.
Zoom in:

```{r}
#| vscode: {languageId: r}
plot(x,0*x,ylim=c(-.2,.2), type = 'l')
matlines(x,X6p[,2:7],lwd = 2)
grid(col = 3) #6 polynomials
#othornormal basis set ==> much numerical stability for our model
```

These basis functions are  all uncorrelated. That is a consequence (and in fact essentially equivalent) to being orthogonal. 

```{r}
#| vscode: {languageId: r}
print(round(cor(X6p[,2:7]),2))
```

Their numerical properties are also better.

```{r}
#| vscode: {languageId: r}
A = t(X6p)%*%X6p
B = solve(A)
max(abs(A%*%B -diag(rep(1,7)))) #should be close to 0
```

```{r}
#| vscode: {languageId: r}
fit=lm(y ~ poly(x,6))
print(summary(fit)) #what do you notice? Which predictors are worth keeping in the model?remeber what the data looks like? does this makes sense?
```

### Example 1

We’ll use the Boston data set [in MASS package], for predicting the median house value (mdev), in Boston Suburbs, based on the predictor variable lstat (percentage of lower status of the population).

We’ll randomly split the data into training set (80% for building a predictive model) and test set (20% for evaluating the model). Make sure to set seed for reproducibility.

```{r}
#| vscode: {languageId: r}
# Load the data
data("Boston", package = "MASS")
# Split the data into training and test set
set.seed(123)
training.samples <- Boston$medv %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- Boston[training.samples, ]
test.data <- Boston[-training.samples, ]
```

First, visualize the scatter plot of the medv vs lstat variables as follow:

```{r}
#| vscode: {languageId: r}
ggplot(train.data, aes(lstat, medv) ) +
  geom_point() +
  stat_smooth()
```

Let's compare the different models in order to choose the best one for our data.

The standard linear regression model equation can be written as medv = b0 + b1*lstat.

(a). Compute linear regression model:

```{r}
#| vscode: {languageId: r}
# Build the model
model <- lm(medv ~ lstat, data = train.data)
# Make predictions
predictions <- model %>% predict(test.data)
# Model performance
print(data.frame(
  RMSE = RMSE(predictions, test.data$medv),
  R2 = R2(predictions, test.data$medv)
))
```

Visualize the data:

```{r}
#| vscode: {languageId: r}
ggplot(train.data, aes(lstat, medv) ) +
  geom_point() +
  stat_smooth(method = lm, formula = y ~ x)
```

(b). Polynomial regression

The polynomial regression adds polynomial or quadratic terms to the regression equation as follow:

$medv=b0+b1*lstat+b2*lstat^2$

In R, to create a predictor x^2 you should use the function I(), as follow: I(x^2). This raise x to the power 2.

```{r}
#| vscode: {languageId: r}
lm(medv ~ lstat + I(lstat^2), data = train.data)
```

An alternative simple solution is to use this:

```{r}
#| vscode: {languageId: r}
lm(medv ~ poly(lstat, 2, raw = TRUE), data = train.data)
```

The output contains two coefficients associated with lstat : one for the linear term (lstat^1) and one for the quadratic term (lstat^2).

The following example computes a sixfth-order polynomial fit:

```{r}
#| vscode: {languageId: r}
lm(medv ~ poly(lstat, 6, raw = TRUE), data = train.data) %>%
  summary()
```

From the output above, it can be seen that polynomial terms beyond the fith order are not significant. So, just create a fith polynomial regression model as follow:

```{r}
#| vscode: {languageId: r}
# Build the model
model <- lm(medv ~ poly(lstat, 5, raw = TRUE), data = train.data)
# Make predictions
predictions <- model %>% predict(test.data)
# Model performance
print(data.frame(
  RMSE = RMSE(predictions, test.data$medv),
  R2 = R2(predictions, test.data$medv)
))
```

Visualize the fith polynomial regression line as follow:

```{r}
#| vscode: {languageId: r}
ggplot(train.data, aes(lstat, medv) ) +
  geom_point() +
  stat_smooth(method = lm, formula = y ~ poly(x, 5, raw = TRUE))
```

## Cubic Splines and Smoothing Splines

Cubic spline with fixed knots.

```{r}
#| vscode: {languageId: r}
#library(splineDesign)
attach(Wage)
agelims=range(Wage$age)
age.grid=seq(from=agelims[1],to=agelims[2])
spline_fit <- lm(wage ~ bs(age, knots=c(20,40,60)), data=Wage) 

plot(Wage$age, Wage$wage, pch=19, col='grey')
pred = predict(spline_fit,list(age=age.grid), se=T)
lines(age.grid,pred$fit, col="#3690C0",lwd=4)
```

 A natural spline has better behavior at the boundary points.

```{r}
#| vscode: {languageId: r}
agelims=range(Wage$age)
age.grid=seq(from=agelims[1],to=agelims[2])

spline_fit <- lm(wage ~ ns(age, knots=c(20,40,60)), data=Wage)
plot(Wage$age, Wage$wage, pch=19, col='grey')
pred = predict(spline_fit,list(age=age.grid), se=T)
lines(age.grid,pred$fit, col="#3690C0",lwd=4)
```

A smoothing spline is a cubic spline with a knot at every observed $x$ but also a penalization to encourage smoothness.

```{r}
#| vscode: {languageId: r}
plot(Wage$age, Wage$wage, pch=19, col='grey')
ss_fit <- smooth.spline(Wage$age,Wage$wage,cv=TRUE) 
lines(ss_fit,col="firebrick", lwd=4)
```

### Example 2

For our previous example, we’ll place the knots at the lower quartile, the median quartile, and the upper quartile:

```{r}
#| vscode: {languageId: r}
knots <- quantile(train.data$lstat, p = c(0.25, 0.5, 0.75))
```

We’ll create a model using a cubic spline (degree = 3):

```{r}
#| vscode: {languageId: r}
# Build the model
knots <- quantile(train.data$lstat, p = c(0.25, 0.5, 0.75))
model <- lm (medv ~ bs(lstat, knots = knots), data = train.data)
# Make predictions
predictions <- model %>% predict(test.data)
# Model performance
print(data.frame(
  RMSE = RMSE(predictions, test.data$medv),
  R2 = R2(predictions, test.data$medv)
))
```

Note that, the coefficients for a spline term are not interpretable.

Visualize the cubic spline as follow:

```{r}
#| vscode: {languageId: r}
ggplot(train.data, aes(lstat, medv) ) +
  geom_point() +
  stat_smooth(method = lm, formula = y ~ splines::bs(x, df = 3))
```

## GAM 

```{r}
#| vscode: {languageId: r}
# Build the model
model <- gam(medv ~ s(lstat), data = train.data)
# Make predictions
predictions <- model %>% predict(test.data)
# Model performance
print(data.frame(
  RMSE = RMSE(predictions, test.data$medv),
  R2 = R2(predictions, test.data$medv)
))
```

```{r}
#| vscode: {languageId: r}
ggplot(train.data, aes(lstat, medv) ) +
  geom_point() +
  stat_smooth(method = gam, formula = y ~ s(x))
```

From analyzing the RMSE and the R2 metrics of the different models, it can be seen that the polynomial regression, the spline regression and the generalized additive models outperform the linear regression model and the log transformation approaches.

### From the book

```{r}
#| vscode: {languageId: r}
gam1=lm(wage~ns(year,4)+ns(age,5)+education,data=Wage)

gam.m3=gam(wage~s(year,4)+s(age,5)+education,data=Wage)

summary(gam.m3)

plot(gam.m3, se=TRUE,col="blue")
```

```{r}
#| vscode: {languageId: r}
gam.m1=gam(wage~s(age,5)+education,data=Wage)
gam.m2=gam(wage~year+s(age,5)+education,data=Wage)
gam.m3=gam(wage~s(year,4)+s(age,5)+education,data=Wage)
print(anova(gam.m1,gam.m2,gam.m3,test="F"))

preds=predict(gam.m2,newdata=Wage)

gam.lo=gam(wage~s(year,df=4)+lo(age,span=0.7)+education,data=Wage)
plot(gam.lo, se=TRUE, col="green")
```

## LOESS

```{r}
#| vscode: {languageId: r}
library(tidyverse)
library(dslabs)
library(caret)

data("polls_2008")
qplot(day, margin, data = polls_2008)

total_days <- diff(range(polls_2008$day))
span <- 21/total_days

fit <- loess(margin ~ day, degree=1, span = span, data=polls_2008)

polls_2008 %>% mutate(smooth = fit$fitted) %>%
  ggplot(aes(day, margin)) +
  geom_point(size = 3, alpha = .5, color = "grey") +
  geom_line(aes(day, smooth), color="red")
```

### From the book

```{r}
#| vscode: {languageId: r}
attach(Wage)
plot(age,wage,xlim=agelims ,cex=.5,col="darkgrey")
fit=loess(wage~age,span=.2,data=Wage)
fit2=loess(wage~age,span=.5,data=Wage)
lines(age.grid,predict(fit,data.frame(age=age.grid)),col="red",lwd=2)
lines(age.grid,predict(fit2,data.frame(age=age.grid)),col="blue",lwd=2)
legend("topright",legend=c("Span=0.2","Span=0.5"),col=c("red","blue"),lty=1,lwd=2,cex=.8)
```

# Part-2: Lab-Assignment


## Question-1: 

Please write a paragraph about what you can conclude from this example (i.e. final Conclusions).  

### S&P Data: Discuss this with your group members 

We will working with the weekly S&P500 data.

```{r}
#| vscode: {languageId: r}
print(names(Weekly))
print(head(Weekly))
attach(Weekly) #time series data
```

We want to predict weekly Volume from performance in previous weeks. 

Make an additive model that uses polynomial regression for each of the predictors.

```{r}
#| vscode: {languageId: r}
library(gam)
fit4 = gam(Volume ~ poly(Lag1,3 ) + poly(Lag2,3 ) + poly(Lag3,3 )) #polynomials of lag features
summary(fit4)
par(mfrow=c(1,3)) #to partition the Plotting Window
plot(fit4,se = TRUE)
```

Plot the data and the fits. 

```{r}
#| vscode: {languageId: r}
plot(Volume,col="darkgrey",type="l")
preds.Weekly=predict(fit4,se=TRUE)
lines(preds.Weekly$fit,lwd=2,col="blue") #need to treat this as time series data
```

Clearly the fits have nothing to do with the data. It is essential to include time in the fit. Also, the exponential increase of the trading volume suggests that one should look at the log of the volume.

```{r}
#| vscode: {languageId: r}
plot.ts(log10(Volume), type="l")
```

There is a general linear trend, the magnitude of the variation is about the same for all years, there are deviations from the linear trend around the years 2002 and 2008, and there may be seasonal variation. 


### Smoothing splines

 We can use smoothing splines to summarize these data. For very large penalty parameters $\lambda$ or equivalently $df = 2$, a smoothing spline is essentially a straight line. We'll make such a smoothing spline, compute predictions, and plot it. Choosing a large lambda is equivalent to using two degrees of freedom (two parameters are fitted).

```{r}
#| vscode: {languageId: r}
fit.s1 = smooth.spline(log10(Volume) ~  1:1089, df = 2) #constraint or flexible with df=2? linear?
preds.s1 = predict(fit.s1)
plot.ts(log10(Volume), col = "darkgrey")
lines(preds.s1$y, lwd =2 ) #high bias?low bias?
```

To capture year-to-year variation, increase the number of degrees of freedom. Let's use one df per year, plus one for the intercept. This is plotted in red.

```{r}
#| vscode: {languageId: r}
fit.s1 = smooth.spline(log10(Volume) ~  1:1089, df = 2)
preds.s1 = predict(fit.s1)
fit.s22 = smooth.spline(log10(Volume) ~  1:1089, df = 22)
preds.s22 = predict(fit.s22)

plot(log10(Volume), col = "darkgrey")
lines(preds.s1$y, lwd =2 )
lines(preds.s22$y, lwd =2, col = 2)
```

To capture also seasonal variation, increase the number of degrees of freedom further. We use four df per year, plus one for the intercept. This is plotted in blue.

```{r}
#| vscode: {languageId: r}
fit.s85 = smooth.spline(log10(Volume) ~  1:1089, df = 85)
preds.s85 = predict(fit.s85)

plot(log10(Volume), col = "darkgrey")
lines(preds.s85$y, lwd =2, col = 4) #what can you say about the overall market trend?
```

When the number of degrees of freedom is not specified, \texttt{gam} chooses one. Here is the resulting plot. This fit uses about 120 degrees of freedom.

```{r}
#| vscode: {languageId: r}
fit.s = smooth.spline(log10(Volume) ~  1:1089)
preds.s = predict(fit.s)

plot(log10(Volume), col = "darkgrey")
lines(preds.s$y, lwd =2, col = 4)
fit.s
```

### Solution: 

INSERT FINAL CONCLUSIONS HERE

## Question-2
This question relates to the College data set.

(a) Split the data into a training set and a test set. Using out-of-state tuition as the response and the other variables as the predictors, perform forward stepwise selection on the training set in order to identify a satisfactory model that uses just a subset of the predictors. Plot the BIC to pick the best model (cross validation could also be used).

(b) Fit a GAM on the training data, using out-of-state tuition as the response and the features selected in the previous step as the predictors. Plot the results, and explain your findings.

(c) Evaluate the model obtained on the test set, and explain the results obtained.

(d) For which variables, if any, is there evidence of a non-linear relationship with the response? (make scatterplots of the response against the five numerical predictors.)


