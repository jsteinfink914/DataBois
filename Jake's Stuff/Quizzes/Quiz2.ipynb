{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: 'Quiz 2'\n",
    "author: Nakul R. Padalkar\n",
    "format:\n",
    "  html:\n",
    "    embed-resources: true\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold Cross Validation\n",
    "## Question - 1: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-fold CV is used to evaluate ML model performance. Here, we split the data into k subsets or folds of equal size. We use k-1 of the subsets as the training data and use the last fold as the test or validation set. We repeat this process k times so that each fold becomes the test set once. For each of these runs we collect a test error metric like RMSE, and at the end we compute the average of these errors to estimate the performance of the model.\n",
    "\n",
    "The validation set approach is essentially K-fold CV but we only do the first run. As a result, the model never gets to see the data in the test fold whereas in K-fold CV the model gets to see all the data in the training process at some point. As a result, it is not as robust of an estimate of model performance returning only one round of test error instead of an average over k rounds.\n",
    "\n",
    "LOOCV is K-fold CV when k = n(the number of rows in the dataset). LOOCV is arguably the most robust estimate of model performance due to it averaging n test errors, but this method can be prohibitively expensive computationally. With a large dataset of n > 10,000+ the benefits of this method are substantially outweighed by the costs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question - 2: \n",
    "### 2.1 - Generate and plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(100)\n",
    "X <- rnorm(500)\n",
    "Y <- 5.3 + 12*X - 3.41*(X**2)\n",
    "xy <- data.frame(X, Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - LOOCV using least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(modeldata)\n",
    "library(leaps)\n",
    "library(caret)\n",
    "library(corrplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>Model 1</dt><dd>3.5299</dd><dt>Model 2</dt><dd>0</dd><dt>Model 3</dt><dd>0</dd><dt>Model 4</dt><dd>0</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Model 1] 3.5299\n",
       "\\item[Model 2] 0\n",
       "\\item[Model 3] 0\n",
       "\\item[Model 4] 0\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Model 1\n",
       ":   3.5299Model 2\n",
       ":   0Model 3\n",
       ":   0Model 4\n",
       ":   0\n",
       "\n"
      ],
      "text/plain": [
       "Model 1 Model 2 Model 3 Model 4 \n",
       " 3.5299  0.0000  0.0000  0.0000 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Make CV function with specified models\n",
    "CV <- function(data, k){\n",
    "  cutoff <- floor(nrow(data)/k)\n",
    "  mse_1 <- c()\n",
    "  mse_2 <- c()\n",
    "  mse_3 <- c()\n",
    "  mse_4 <- c()\n",
    "  for (i in 1:k){\n",
    "    start <- 1 + ((i-1) * cutoff)\n",
    "    stop <- start + cutoff - 1\n",
    "    train <- data[!1:nrow(data) %in% start:stop,]\n",
    "    test <- data[start:stop,]\n",
    "    model_1 <- lm(Y ~ X, data = train)\n",
    "    model_2 <- lm(Y ~ poly(X,2,raw = T), data = train)\n",
    "    model_3 <- lm(Y ~ poly(X,3,raw = T), data = train)\n",
    "    model_4 <- lm(Y ~ poly(X,4,raw = T), data = train)\n",
    "    predict_1 <- predict(model_1, test)\n",
    "    predict_2 <- predict(model_2, test)\n",
    "    predict_3 <- predict(model_3, test)\n",
    "    predict_4 <- predict(model_4, test)\n",
    "    mse_1 <- c(mse_1, RMSE(predict_1, test$Y))\n",
    "    mse_2<- c(mse_2, RMSE(predict_2, test$Y))\n",
    "    mse_3 <- c(mse_3, RMSE(predict_3, test$Y))\n",
    "    mse_4 <- c(mse_4, RMSE(predict_4, test$Y))\n",
    "  }\n",
    "  return(c(\"Model 1\" = round(mean(mse_1),4), \"Model 2\" = round(mean(mse_2),4), \"Model 3\" = round(mean(mse_3),4),\n",
    "  \"Model 4\" = round(mean(mse_4),4)))\n",
    "}\n",
    "CV(xy, k = nrow(xy))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - 5-fold CV using least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>Model 1</dt><dd>5.3082</dd><dt>Model 2</dt><dd>0</dd><dt>Model 3</dt><dd>0</dd><dt>Model 4</dt><dd>0</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Model 1] 5.3082\n",
       "\\item[Model 2] 0\n",
       "\\item[Model 3] 0\n",
       "\\item[Model 4] 0\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Model 1\n",
       ":   5.3082Model 2\n",
       ":   0Model 3\n",
       ":   0Model 4\n",
       ":   0\n",
       "\n"
      ],
      "text/plain": [
       "Model 1 Model 2 Model 3 Model 4 \n",
       " 5.3082  0.0000  0.0000  0.0000 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CV(xy, k = 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Which model had lowest LOOCV error? As expected? Comment on statistical significance of coefficients and do these results agree with the CV results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## Fitting on first 400 as the \"train\" data - random sampling doesn't matter because they are all generated randomly\n",
    "model_1 <- lm(Y ~ X, data = xy[1:400,])\n",
    "model_2 <- lm(Y ~ poly(X,2,raw = T), data = xy[1:400,])\n",
    "model_3 <- lm(Y ~ poly(X,3,raw = T), data = xy[1:400,])\n",
    "model_4 <- lm(Y ~ poly(X,4,raw = T), data = xy[1:400,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Model 1 coefficients:\"\n",
      "\n",
      "Call:\n",
      "lm(formula = Y ~ X, data = xy[1:400, ])\n",
      "\n",
      "Residuals:\n",
      "    Min      1Q  Median      3Q     Max \n",
      "-34.052  -0.817   2.074   3.000   3.317 \n",
      "\n",
      "Coefficients:\n",
      "            Estimate Std. Error t value Pr(>|t|)    \n",
      "(Intercept)   1.9826     0.2602    7.62 1.87e-13 ***\n",
      "X            12.0425     0.2637   45.66  < 2e-16 ***\n",
      "---\n",
      "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
      "\n",
      "Residual standard error: 5.203 on 398 degrees of freedom\n",
      "Multiple R-squared:  0.8397,\tAdjusted R-squared:  0.8393 \n",
      "F-statistic:  2085 on 1 and 398 DF,  p-value: < 2.2e-16\n",
      "\n",
      "[1] \"Model 2 coefficients:\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in summary.lm(model_2):\n",
      "\"essentially perfect fit: summary may be unreliable\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Call:\n",
      "lm(formula = Y ~ poly(X, 2, raw = T), data = xy[1:400, ])\n",
      "\n",
      "Residuals:\n",
      "       Min         1Q     Median         3Q        Max \n",
      "-1.105e-14 -8.400e-16 -1.570e-16  5.730e-16  4.818e-14 \n",
      "\n",
      "Coefficients:\n",
      "                       Estimate Std. Error    t value Pr(>|t|)    \n",
      "(Intercept)           5.300e+00  1.853e-16  2.860e+16   <2e-16 ***\n",
      "poly(X, 2, raw = T)1  1.200e+01  1.583e-16  7.582e+16   <2e-16 ***\n",
      "poly(X, 2, raw = T)2 -3.410e+00  1.026e-16 -3.325e+16   <2e-16 ***\n",
      "---\n",
      "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
      "\n",
      "Residual standard error: 3.122e-15 on 397 degrees of freedom\n",
      "Multiple R-squared:      1,\tAdjusted R-squared:      1 \n",
      "F-statistic: 3.447e+33 on 2 and 397 DF,  p-value: < 2.2e-16\n",
      "\n",
      "[1] \"Model 3 coefficients:\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in summary.lm(model_3):\n",
      "\"essentially perfect fit: summary may be unreliable\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Call:\n",
      "lm(formula = Y ~ poly(X, 3, raw = T), data = xy[1:400, ])\n",
      "\n",
      "Residuals:\n",
      "       Min         1Q     Median         3Q        Max \n",
      "-1.101e-14 -8.130e-16 -1.500e-16  5.770e-16  4.817e-14 \n",
      "\n",
      "Coefficients:\n",
      "                       Estimate Std. Error    t value Pr(>|t|)    \n",
      "(Intercept)           5.300e+00  1.856e-16  2.856e+16   <2e-16 ***\n",
      "poly(X, 3, raw = T)1  1.200e+01  2.556e-16  4.695e+16   <2e-16 ***\n",
      "poly(X, 3, raw = T)2 -3.410e+00  1.030e-16 -3.310e+16   <2e-16 ***\n",
      "poly(X, 3, raw = T)3 -2.411e-17  5.976e-17 -4.040e-01    0.687    \n",
      "---\n",
      "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
      "\n",
      "Residual standard error: 3.125e-15 on 396 degrees of freedom\n",
      "Multiple R-squared:      1,\tAdjusted R-squared:      1 \n",
      "F-statistic: 2.293e+33 on 3 and 396 DF,  p-value: < 2.2e-16\n",
      "\n",
      "[1] \"Model 4 coefficients:\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in summary.lm(model_4):\n",
      "\"essentially perfect fit: summary may be unreliable\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Call:\n",
      "lm(formula = Y ~ poly(X, 4, raw = T), data = xy[1:400, ])\n",
      "\n",
      "Residuals:\n",
      "       Min         1Q     Median         3Q        Max \n",
      "-1.107e-14 -8.160e-16 -2.310e-16  5.550e-16  4.798e-14 \n",
      "\n",
      "Coefficients:\n",
      "                       Estimate Std. Error    t value Pr(>|t|)    \n",
      "(Intercept)           5.300e+00  2.111e-16  2.511e+16   <2e-16 ***\n",
      "poly(X, 4, raw = T)1  1.200e+01  2.562e-16  4.683e+16   <2e-16 ***\n",
      "poly(X, 4, raw = T)2 -3.410e+00  2.463e-16 -1.384e+16   <2e-16 ***\n",
      "poly(X, 4, raw = T)3 -4.733e-17  6.083e-17 -7.780e-01   0.4370    \n",
      "poly(X, 4, raw = T)4  6.721e-17  3.566e-17  1.885e+00   0.0602 .  \n",
      "---\n",
      "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
      "\n",
      "Residual standard error: 3.115e-15 on 395 degrees of freedom\n",
      "Multiple R-squared:      1,\tAdjusted R-squared:      1 \n",
      "F-statistic: 1.731e+33 on 4 and 395 DF,  p-value: < 2.2e-16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Model 1 coefficients:\")\n",
    "print(summary(model_1))\n",
    "print(\"Model 2 coefficients:\")\n",
    "print(summary(model_2))\n",
    "print(\"Model 3 coefficients:\")\n",
    "print(summary(model_3))\n",
    "print(\"Model 4 coefficients:\")\n",
    "print(summary(model_4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models 2,3, and 4 all had 0 RMSE error, which at first shocked me, but now it makes sense looking at the variable significances. The data was based on a clear cut ground truth equation with 0 noise added to it. As a result, the models with the appropriate degree polynomials (2+) were able to map perfectly onto the data. Those with excess degrees simply put the coefficients at essentially 0 and were not significant, erasing their effects. This is what we see above looking at the model summaries, with the parameters mapping exactly to the ground truth and being significant up to $X^2$. Based on these model summaries, I can now see why the CV evaluations for models 2+ showed 0 error - because they mapped exactly correctly to the ground truth."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset selection\n",
    "## Question - 3:\n",
    "\n",
    "Best subset selection means trying all possible subsets of features and selecting the best model based on metrics like AIC, BIC, and AdjR2, which are estimates of the test error. Stepwise subset selection follows this same approach of trying models and evaluating based on the same metrics, however the big difference is how it selects subsets. Stepwise subset selection does not try all possible models, it either adds or removes features one at a time based on what improves the model most. This means that instead of trying all possible models after a feature is added or removed, it will only try the models with one more or one less feature and see what does best, then choosing that model to work on next. Once it can no longer improve significantly it will stop. The two main types are forward and backward selection.\n",
    "\n",
    "## Question - 4:\n",
    "\n",
    "These metrics are all used to measure model fit. \n",
    "\n",
    "RSS (residual sum of squares) measures the squared difference between predicted and actual values. A lower RSS means better fit. This method emphasizes outliers due to the squaring, and thus will have a higher RSS than some other methods if there are a few predictions that are way off.\n",
    "\n",
    "AIC balances model complexity with goodness of fit. It penalizes more parameters in a model and also takes into account the accuracy of the model. A model with low complexity and low RSS will have a low AIC.\n",
    "\n",
    "BIC is similar to AIC but puts a harsher penalty on model complexity. It can also be more effective in smaller sample sizes as it is derived more rigorously from bayesian theory. Oftentimes it will have a minimum value for a less complex model than what other metrics would suggest.\n",
    "\n",
    "$C_p$ also balances model complexity and goodness of fit, taking into account MSE and the number of parameters. It compares the MSE of the model to the SSE of a model with all the predictors.\n",
    "\n",
    "## Question - 5:\n",
    "\n",
    "RSS is a measure of total model fit irrespective of complexity. It is a blunt measure that only cares about the relationship between predictions and actual values. AIC, BIC, and $C_p$ all balance model complexity and performance. BIC is the strictest of these on model complexity and better in small sample sizes. AIC is the same as BIC but less harsh and less good under small sample sizes. $C_p$ uses the MSE to estimate accuracy which is different then the other methods, and balances MSE, parameters, and sample size to generate a score. It is important to note that AIC, BIC and $C_p$ are used to evaluate model performance while RSS is usually not.\n",
    "\n",
    "## Question - 6:\n",
    "\n",
    "$R^2$ represents the proportion of RSS/TSS or the proportion of the variance in target values that is explained by the features. The more variation the features explain the higher $R^2$ is because the formula is 1 - RSS/TSS. The values only span from 0-1. \n",
    "\n",
    "$R^2_{adj}$ is the same as $R^2$ except it adds extra parameters to adjust for the number of parameters, pushing the score lower as more are added. As a result, it is a better metric and yields models less prone to overfitting. However, they are equivalently useful metrics when evaluating models with the same number of parameters.\n",
    "\n",
    "## Question - 7:\n",
    "\n",
    "RSS and $R^2$ metrics are not suitable for subset selection procedures because these procedures rely on looking at models with different numbers of parameters and comparing them. These metrics do not take into account model complexity and thus provide little usefulness in pursuit of finding the best, simplest model. These metrics will simply show you the best model, but not the simplest. We want a simpler model because they are less prone to overfitting on unseen data.\n",
    "\n",
    "The other grouping of metrics are suitable because they account for model complexity as well as accuracy. They each do so in their own way with BIC being the most strict on complexity, but they all impose a penalty for more parameters, the difference is only in how they impose it. As a result, when comparing models of different complexity, we use these metrics to examine the models. However, it is important to note that it is best practice to use more than one of these metrics when comparing models as they can, and often do, provide different results.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
